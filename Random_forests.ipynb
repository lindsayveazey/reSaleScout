{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Going for a [walk](https://xcitech.github.io/tutorials/heroku_tutorial/) in the (random) forest.*\n",
    "\n",
    "# Random forests\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import *\n",
    "import pickle\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data\n",
    "Recall that low_memory=False commands the module to read in the file before assigning data types. Not using it now, may later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = pd.read_csv('cleaned_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Summary column- not useful at this point- and auto-generated indexing column ('Unnamed: 0')\n",
    "cleaned_df = cleaned_df.drop(['Unnamed: 0'], axis=1)\n",
    "cleaned_df = cleaned_df.drop(['Summary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Brand</th>\n",
       "      <th>Condition</th>\n",
       "      <th>Category</th>\n",
       "      <th>Site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>48.00</td>\n",
       "      <td>lululemon</td>\n",
       "      <td>New</td>\n",
       "      <td>bottoms</td>\n",
       "      <td>eBay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>41.00</td>\n",
       "      <td>lululemon</td>\n",
       "      <td>New</td>\n",
       "      <td>bottoms</td>\n",
       "      <td>eBay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>16.00</td>\n",
       "      <td>lululemon</td>\n",
       "      <td>New</td>\n",
       "      <td>tops</td>\n",
       "      <td>eBay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>37.95</td>\n",
       "      <td>lululemon</td>\n",
       "      <td>New</td>\n",
       "      <td>bottoms</td>\n",
       "      <td>eBay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>15.00</td>\n",
       "      <td>lululemon</td>\n",
       "      <td>New</td>\n",
       "      <td>bras</td>\n",
       "      <td>eBay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Price      Brand Condition Category  Site\n",
       "0  48.00  lululemon       New  bottoms  eBay\n",
       "1  41.00  lululemon       New  bottoms  eBay\n",
       "2  16.00  lululemon       New     tops  eBay\n",
       "3  37.95  lululemon       New  bottoms  eBay\n",
       "4  15.00  lululemon       New     bras  eBay"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "Here, I take categorical data and convert it to arbitrary numerical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the data using pandas get_dummies\n",
    "cleaned_df = pd.concat([cleaned_df,pd.get_dummies(cleaned_df['Brand'],drop_first=True,prefix=\"Brand\")],axis=1)\n",
    "cleaned_df = pd.concat([cleaned_df,pd.get_dummies(cleaned_df['Condition'],drop_first=True,prefix=\"Condition\")],axis=1)\n",
    "cleaned_df = pd.concat([cleaned_df,pd.get_dummies(cleaned_df['Category'],drop_first=True,prefix=\"Category\")],axis=1)\n",
    "cleaned_df = pd.concat([cleaned_df,pd.get_dummies(cleaned_df['Site'],drop_first=True,prefix=\"Site\")],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then remove the original columns, which all contain strings. The model can't use those, thus the bother with OHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.drop(['Brand', 'Condition', 'Category', 'Site'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/testing split\n",
    "\n",
    "I am setting the random state (equivalent to set.seed in the R universe) to 29- which means the results will be the same each time I run the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_df.drop('Price', axis=1), \n",
    "                                                                            cleaned_df['Price'], test_size = 0.2, \n",
    "                                                                            random_state = 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (48356, 7)\n",
      "Training Labels Shape: (48356,)\n",
      "Testing Features Shape: (12090, 7)\n",
      "Testing Labels Shape: (12090,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', X_train.shape)\n",
    "print('Training Labels Shape:', y_train.shape)\n",
    "print('Testing Features Shape:', X_test.shape)\n",
    "print('Testing Labels Shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For another time, perhaps- dealing with time series, or [cyclical features](http://blog.davidkaleko.com/feature-engineering-cyclical-features.html).*\n",
    "\n",
    "Moving on...\n",
    "\n",
    "### Establish baseline\n",
    "\n",
    "The baseline is the error I would get if I simply predicted the historical average sale price for all items. If I can reduce the error by using my model, that's a good sign the approach is valid.\n",
    "\n",
    "I'll now separate the data into the feature/independent/predictor variables and target/label/dependent/predicted variable. \n",
    "\n",
    "I'll also convert the df to numpy arrays for easily digestion by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average baseline error: $ 33.86\n"
     ]
    }
   ],
   "source": [
    "# Create an array representing the y (dependent variable) values, also known as labels\n",
    "labels = np.array(cleaned_df['Price']) \n",
    "\n",
    "# Remove the labels from the x (independent variable) values, also known as features\n",
    "# axis = 1 refers to the columns\n",
    "features = cleaned_df.drop('Price', axis = 1)\n",
    "\n",
    "# Saving feature names to a list for use in a moment\n",
    "feature_list = list(cleaned_df.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "baseline_preds = test_features[:, feature_list.index('Price')]\n",
    "\n",
    "# Baseline errors, and display average baseline error\n",
    "baseline_errors = abs(baseline_preds - \n",
    "                      test_labels)\n",
    "print('Average baseline error: $', round(np.mean(baseline_errors), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "                      max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                      n_jobs=None, oob_score=False, random_state=29, verbose=0,\n",
       "                      warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 29)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting to the withheld data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: $ 12.58\n"
     ]
    }
   ],
   "source": [
    "#Predicting on the Test Set\n",
    "predictions = rf.predict(X_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error: $', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hot damn, an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle that\n",
    "I messed up *a lot* here. If you're adapting this tutorial, pay attention to all the little details that come next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.51278380621452\n"
     ]
    }
   ],
   "source": [
    "# My model is called 'rf' (as you see above)- whatever you called your model, substitute that in for 'rf', below.\n",
    "# Don't change anything else unless you really want to.\n",
    "with open('model.pkl', 'wb') as fid:\n",
    "    pickle.dump(rf, fid,2)  \n",
    "\n",
    "# Load the model from disk\n",
    "loaded_model = pickle.load(open('model.pkl', 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty terrible performance! \n",
    "\n",
    "Moving on..\n",
    "\n",
    "We need to create our feature vector of exact same dimension as our training set. To convert our user input into dummy variables, we should save a dict of the the dummy variables. Later we can populate our feature vector for prediction using this dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe with only the dummy variables. Mine was called 'cleaned_df' and my dependent variable is 'Price'.\n",
    "\n",
    "cat = cleaned_df.drop('Price',axis=1)\n",
    "index_dict = dict(zip(cat.columns,range(cat.shape[1])))\n",
    "\n",
    "with open('cat', 'wb') as fid:\n",
    "    pickle.dump(index_dict, fid, 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60446, 7)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of model performance\n",
    "\n",
    "Super, model has run, but is it decent? To check, I'm calculating accuracy by substracting the mean average percentage error (MAPE) from 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39.01 %.\n"
     ]
    }
   ],
   "source": [
    "mape = 100 * (errors / y_test)\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is kind of bad. I'll need to tune, I think.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model tuning\n",
    "I'm saving [this](https://scikit-learn.org/stable/modules/grid_search.html) for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting results: variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: Category_bras        Importance: 0.56\n",
      "Variable: Category_outerwear   Importance: 0.2\n",
      "Variable: Category_tops        Importance: 0.07\n",
      "Variable: Price                Importance: 0.06\n",
      "Variable: Brand_lululemon      Importance: 0.06\n",
      "Variable: Condition_PreOwned   Importance: 0.05\n",
      "Variable: Category_dresses     Importance: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er, not much looking great here, but it just tells me I need more data, probably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: $ 13.53\n",
      "Accuracy: 33.66 %.\n"
     ]
    }
   ],
   "source": [
    "# New random forest with only the two most important variables\n",
    "rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=22)\n",
    "\n",
    "### EDIT THIS NEXT SECTION\n",
    "# Extract the two most important features\n",
    "important_indices = [feature_list.index('Category_bras'), feature_list.index('Category_outerwear')]\n",
    "train_important = train_features[:, important_indices]\n",
    "test_important = test_features[:, important_indices]\n",
    "\n",
    "# Train the random forest\n",
    "rf_most_important.fit(train_important, train_labels)\n",
    "\n",
    "# Make predictions and determine the error\n",
    "predictions = rf_most_important.predict(test_important)\n",
    "\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "# Display the performance metrics\n",
    "print('Mean Absolute Error: $', round(np.mean(errors), 2))\n",
    "\n",
    "mape = np.mean(100 * (errors / test_labels))\n",
    "\n",
    "accuracy = 100 - mape\n",
    "\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worse! Oh god, it's even worse. Oh dear. Well, I just need more data, this is not surprising, I expected it. Onwards!\n",
    "\n",
    "You may, at this point, want to open the troubleshooting_scratchpad.ipynb for instructions on how to use your pickled model to talk to the Flask app."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
